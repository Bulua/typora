[toc]



# 摘要

**自动视觉字幕(Automatic Visual Captioning，AVC)**通过描述重要的对象、属性以及它们之间的关系来生成语法和语义上正确的句子。它分为两类:图像字幕和视频字幕。广泛应用于视障辅助、人机交互、视频监控系统、场景理解等领域。随着深度学习在计算机视觉和自然语言处理领域的空前成功，这一领域的研究在过去的几年中掀起了一场热潮。在这项调查中，根据他们如何概念化字幕问题，即传统的视觉描述方法，要么是检索，要么是基于模板的描述和深度学习方法，分类的最新技术。详细回顾了现有的方法，突出了它们的优缺点，社会影响的引用数量，使用的架构，实验数据集和GitHub链接。此外，该调查还概述了基准图像和视频数据集，以及为评估机器生成字幕的质量而开发的评估措施。研究发现，密集或段落生成和改变图像字幕(CIC)由于其接近人的抽象能力而更受研究界的欢迎。最后，本文探讨了自动视觉字幕生成领域的未来发展方向。

# 一、视觉字幕技术

**视觉字幕**是近年来流行起来的一种基于视觉理解和内容总结的视频字幕。视觉内容具有图像和视频形式的数据，其中包含特定的主题或更具体地说，一个场景、一个事件等。视觉内容被转换成文字说明，而文字说明只不过是单词或字符串。视觉字幕分为**图像字幕**和**视频字幕**，**视频字幕**又分为单句字幕和密集字幕。图像字幕为每一帧生成一个句子，而与视频字幕相比，生成的是一个包含一句话的完整视频的描述。在密集图像字幕中，提取每一帧的特征，并以更丰富的语义感知句子的形式表示出来;而在密集视频字幕中，对每一帧视频进行时间检测和描述，从而利用帧中的空间和语义细节对整个视频进行密集描述。

视频字幕大致分为两类，如图所示:**(i)单句视频字幕(SSVC)， (ii)密集视频字幕(DVC)**。SSVC涉及到用一句话描述整个视频，这可能不足以描述整个视频。基于CNN或RNN的模型通常被用来用一句话描述一个视频。而DVC生成多个紧密相连的自然语言句子。在这个过程中，事件需要在时间上进行本地化。此外，在密集的标题中生成的描述更详细，可能以段落的形式出现。

![20231010115632](https://raw.githubusercontent.com/Bulua/BlogImageBed/master/20231010115632.png)

## 1.1 单句视频字幕(SSVC)

自动化的单句视频字幕(SSVC)以单句的形式生成整个视频的内容。SSVC大致分为==基于**模板**、基于**机器学习**和基于**深度学习**==的方法。

### 1.1.1 模板方法

该技术首先检测视频的所有相关操作、实体和事件，然后创建用于生成合适描述的模板。这些描述在语法上是正确的，但是，这种技术在生成变长句子描述时缺乏。它检测视频中的主语、动词和宾语(SVO)，并将它们放入预定义的模板中。图19 (a)为一些常用的句子生成模板示例，这些模板优先使用时空特征获取动词，使用对象检测方法获取主语和宾语。在SVO中，动词由利用时空特征的动作或活动检测方法获得，而主语和宾语则由利用空间特征的对象检测方法获得。大多数基于模板的方法由以下两部分组成:**(1)视觉属性识别;(2)自然语言描述生成**。句子是用句法结构生成的，句子的质量在很大程度上取决于句子的模板。图19 (b)展示了用于句子生成的样本谓词。

![20231010115702](https://raw.githubusercontent.com/Bulua/BlogImageBed/master/20231010115702.png)

经典的基于模板的方法。首先检测可能的人或物体、动作和场景，然后根据主语（S）-动词（V）-宾语（O）模板生成句子，以便于人类阅读。但由于字幕模板的依赖性很强，句子结构固定，无法满足字幕灵活多样的要求。

### 1.1.2 机器学习方法

基于**机器学习（Machine Learning，ML）**的视频字幕模型使得视觉跟踪和识别更加健壮。这些方法比基于模板的模型提供了更好的机会来处理更大的数据集。它在图像处理、活动和目标检测方面采用了更好的策略。

### 1.1.3 深度学习方法

前两小节中讨论的方法显示了一种实用的视频字幕生成方法，因为生成的句子非常刻板，词汇量也非常少。因此，基于svo的方法不适用于开放域数据集。基于深度学习的视频字幕技术处理基于神经网络的算法，这些算法识别动作和检索信息，证明了神经网络的有效性。这些算法有利于学习表示形式，而不需要直接从输入数据中提取特征。图20所示为基于深度学习的视频字幕基本结构表示，包括两个阶段，即编码阶段(视觉内容提取)和解码阶段(语言生成阶段)。用于提取特定视频特征的流行技术包括CNN， RNN，或LSTM。

![20231010115725](https://raw.githubusercontent.com/Bulua/BlogImageBed/master/20231010115725.png)

基于cnn的体系结构为视觉数据的表示提供了最先进的建模技术，而LSTM和RNN则在机器翻译(MT)领域树立了新的基准。第一阶段使用的技术也可以用于语言的生成或具有不同RNN结构的解码阶段，如bi-RNN、LSTM或GRU。AlexNet， VGG-16， SPP， GoogleNet等都是可以用于该任务的各种预训练网络。

#### 1.1.3.1 是否包含注意力机制的S-to-S模型

编码器-解码器体系结构是基于序列到序列的视频字幕模型的基础，该模型主要包括两个部分:(1)从视频中提取特征，(2)生成自然语言句子。LSTM被广泛用作语言生成的顺序模型。视频字幕问题涉及到可变长度的输入和输出，可以通过考虑相同大小的输入视频来解决。但是在现实场景中，处理可变长度的输入是很重要的。这个问题已经被许多研究人员以不同的方式提出，比如生成固定长度的视频表示，Pooling over frames（通过对多个连续帧或时间步的信息进行聚合来生成一个池化特征或汇总统计信息），从输入视频中选择固定数量的输入帧，以及处理变长输入。图21 (a)描述了按顺序训练视频帧的模型结构。它由一个描述视频中事件的双层LSTM结构组成，然而，图21 (b)显示了一个类似的结构，除了一个额外的注意层，这对提高生成的描述的性能至关重要。

![20231010115812](https://raw.githubusercontent.com/Bulua/BlogImageBed/master/20231010115812.png)

#### 1.1.3.2 transformer方法

基于transformer的方法最近在广泛的语言任务中表现出典型的性能。transformer网络在NLP领域的突破已经引发了对CV的极大兴趣。这些方法使用一种软注意机制来学习序列元素之间的关系。基于transformer的架构也能够使用注意层堆叠层而不是RNN或CNN处理可变大小的输入，我们可以说这些方法是编码器或解码器层的堆叠层。有研究介绍了使用transformer(BERT)的简单但强大的双向编码器表示。它基于预训练和微调，成功地处理了一系列的NLP任务。VideoBERT是BERT的扩展版本，可用于动作分类和视频字幕等众多任务。它从量化的视频帧中进行学习，并为学习高级语义特征的模型提供量化结果。

#### 1.1.3.3 分层方法

基于层次的视频字幕技术使用时间层次方法或空间层次方法或这些方法的组合来检测视频中对象的出现。视频字幕的分层方法在段落生成领域中得到了应用。

#### 1.1.3.4 深度强化学习的方法

前面讨论的序列到序列模型在提取视频的粗描述方面显示了很有希望的结果，尽管如此，它们仍然不能为包含多个细粒度操作的视频加标题。在这个小节中，我们讨论了基于强化学习的不同技术，这些技术可以生成具有多个细级动作的视频描述。

## 1.2 视频的密集或段落字幕

随着视频长度的增加，视频更可能包含语义复杂度增加的多个事件。因此，单个句子生成多个事件的视频是不够的，增加了语义的复杂性。因此，最近的作品（Yu，Wang，Huang，Yang，& Xu，2016）（Senina，et al.，2014）已经将焦点转移到用于视频的密集字幕或段落生成。基于密集或段落的视频字幕以自然语言的形式描述每个活动和交互。视频中事件的时间定位及其描述涉及密集字幕应用。此外，可能存在的重叠事件的问题通过密集字幕方法来解决。密集或基于段落的视频字幕是视频字幕中的新兴领域，并且预计将在未来几年蓬勃发展，因为它促进了多个事件本地化沿着考虑到事件之间的重叠的每个事件的字幕生成。随着句子描述的密集注释的大规模数据集的引入，这样的问题已经成为可行的解决。

## 1.3 讨论

随着技术和字幕算法的巨大进步，视频字幕的范围也在不断扩大。基于模板的技术是两个阶段的过程，即**视觉属性识别和自然语言生成**。这些方法基于视频中的SVO的检测，并提供严格的字幕，但该技术仅解决了预定义的有限动作，并且不生成有效的视频描述。对于这些方法，评估被限制在具有小词汇的狭窄域。这样的技术仅为视频中的单个人生成字幕，或者只能描述交通中移动的车辆的运动。这些技术不知何故无法处理多人交互，也无法生成流畅且冗长的视频描述。此外，基于ML的技术加强了视觉跟踪和识别，并突出了大词汇量的问题。然而，这种技术缺乏处理更多视觉和声音信息的效率。这里，主要的挑战在于短距离活动检测，其缺乏鲁棒的模型来处理无法描述整个场景的逐帧运动计算的大量计算。随着基于深度学习的SSVC技术的发展，在视觉和语义信息的提取以及高效描述的生成方面观察到了改进。Sequence-to-Sequence模型基于编码器-解码器架构并且使用LSTM作为构建块。这些模型允许可变长度的输入和输出。提高字幕的质量和多样性可以通过基于注意力的机制实现序列到序列学习，该机制考虑了视频的局部和全局时间结构到产品描述。基于变换器的模型生成了更准确的视频描述，这些描述接近实际情况，并克服了LSTM技术的局限性。

为了处理复杂的视频，可以使用时间分层结构和空间分层结构有助于检测视频中对象的出现，并为复杂视频中的大量长距离依赖性提供准确描述。强化学习的优势要大得多，因为它们不限于特定的评估指标，可以提高模型性能并提高生成字幕的多样性。注意到密集或段落视频字幕以自然语言的形式描述视频中的每个活动和交互。很少有研究人员试图将音频和语音模态与视频结合起来，以生成密集的描述，这无疑提高了性能。

# 二、数据集

用于描述视频的标记数据集的可用性是该研究领域快速发展的主要驱动力。在视频数据收集开始时，数据集被分类为**烹饪视频、化妆视频、电影、社交媒体**等。在大多数的数据集中。每个视频由视频的单句描述组成，除了少数数据集包含描述特定视频的多个句子或每个视频片段的段落。下面简要讨论视频字幕数据集：

|   类别   |                            数据集                            |
| :------: | :----------------------------------------------------------: |
| 烹饪视频 | MP-II Cooking Dataset、YouCook Dataset、YouCook-II Dataset、TACoS、HowTo100M dataset、 |
| 化妆视频 |                     HowTo100M dataset、                      |
|   电影   |     MPII-MD dataset、M−VAD Dataset、HowTo100M dataset、      |
| 社交媒体 | VideoStory Dataset、ActivityNet dataset、ActivityNet Entities、MSVD Dataset、MSR-VTT、Charades、VTW、HowTo100M dataset、VATEX |

# 三、评价指标

## 3.1 BLEU

## 3.2 METEOR

## 3.3 ROUGE

## 3.4 CIDEr

































