[toc]



# 1、混淆矩阵

## 1.1 变量介绍

混淆矩阵（Confusion Matrix）是在分类问题中用于评估模型性能的一种表格。它显示了模型的预测结果与实际标签之间的关系，特别是在多类别分类问题中。

对于一个二分类问题，混淆矩阵通常如下所示：

|                   | Actual Class 0 | Actual Class 1 |
| :---------------: | :------------: | :------------: |
| Predicted Class 0 |       TP       |       FP       |
| Predicted Class 1 |       FN       |       TN       |

*初学者容易搞混TP、FP、FN、TN，这里教大家一个方法来记住他们到底怎么来的：第二个字母是你预测的 P 还是 N ，第一个字母是你预测的对不对，对的话就是 T ，不对的话是 N*

在混淆矩阵中：

- True Positive (TP)：模型正确预测为正类别的样本数。
- True Negative (TN)：模型正确预测为负类别的样本数。
- False Positive (FP)：模型错误地将负类别预测为正类别的样本数（误报）。
- False Negative (FN)：模型错误地将正类别预测为负类别的样本数（漏报）。

## 1.2 Accuracy

准确率（Accuracy）是分类问题中常用的性能度量之一，它表示模型==正确预测的样本数==占==总样本数==的比例。准确率可以用以下公式表示：
$$
Accuracy = \frac{TP+TF}{TP+TF+NP+NF}
$$

## 1.3 Precision

Precision（精确率，又称查准率）是分类问题中的一种性能度量，它表示模型在预测为正类别的样本中有多少是真正的正类别。Precision 可以用以下公式表示：
$$
Precision = \frac{TP}{TP+FP}
$$

## 1.4 Recall

Recall（召回率，又称查全率）是分类问题中的一种性能度量，它表示模型在所有真正的正类别中有多少被成功地检测到。Recall 可以用以下公式表示：
$$
Recall = \frac{TP}{TP+FN}
$$
*Precision和Recall是一对矛盾的指标。一般来说，Precision高时，Recall往往偏低；二Recall高时，Precision往往偏低。*

## 1.5 F1-score

F1-score（F1分数）是综合考虑 Precision 和 Recall 的一种性能度量，特别适用于不平衡类别的情况。F1-score 可以通过以下公式计算：
$$
\begin{aligned}
\text{F1-score} &= \frac{2}{\frac{1}{Precision}+\frac{1}{Recall}}	\\
&= \frac{2 \times Precision \times Recall}{Precision+Recall}
\end{aligned}
$$

# 2、损失函数

## 2.1 BCELoss

**BCELoss (Binary Cross Entropy Loss)**，如果模型输出是 `sigmoid `激活函数的概率输出，并且真实标签是 `one-hot` 编码形式，`BCELoss `的计算公式为：
$$
\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],
$$

$$
\ell(x, y) = \begin{cases}
            \operatorname{mean}(L), & \text{if reduction} = \text{`mean';}\\
            \operatorname{sum}(L),  & \text{if reduction} = \text{`sum'.}
        \end{cases}
$$

- 适用于二分类问题，其中每个样本属于两个类别中的一个（正类别或负类别）。
- 输入格式要求是模型输出的概率分布和真实标签的 `one-hot` 编码形式。
- 通常使用 `sigmoid `激活函数来获得概率输出。
- 适用于每个样本只有一个正确类别的情况。

在 `PyTorch `中，`BCELoss` 的默认行为是将每个样本的损失相加，并取平均值。

```python
class torch.nn.BCELoss(weight=None, reduction='mean')
```

主要参数：

1. **weight:** 可选参数，用于指定每个类别的权重。可以是一个 `Tensor`，与目标的形状相同，也可以是一个浮点数。默认值为 `None`，表示所有类别的权重相同。
2. **reduction:** 指定损失的计算方式。可选值为：
   - `none`: 不进行任何降维，保留每个样本的损失（默认）。
   - `mean`: 将每个样本的损失取平均值。
   - `sum`: 将每个样本的损失相加。

## 2.2 CrossEntropyLoss

**CrossEntropyLoss**，如果模型输出是 `softmax `激活函数的概率输出，并且真实标签是整数形式，`CrossEntropyLoss `的计算公式为：
$$
\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
          l_n = - \sum_{c=1}^C w_c y_{n,c} \log \frac{\exp(x_{n,c})}{\sum_{i=1}^C \exp(x_{n,i})} 
$$

- 适用于多分类问题，其中每个样本可以属于多个类别。
- 输入格式要求是模型输出的概率分布和真实标签的整数形式，不需要进行 `one-hot` 编码。
- 通常使用 `softmax `激活函数来获得概率输出。
- 适用于每个样本可以属于多个类别的情况。

在 `PyTorch `中，`CrossEntropyLoss` 的默认行为也是将每个样本的损失相加，并取平均值。

```python
class torch.nn.CrossEntropyLoss(weight=None, ignore_index=-100, reduction='mean')
```

主要参数：

1. **weight:** 可选参数，用于指定每个类别的权重。可以是一个 `Tensor`，与类别数相同，也可以是一个浮点数。默认值为 `None`，表示所有类别的权重相同。
2. **ignore_index:** 如果指定了 `ignore_index`，则忽略此类别的损失计算。默认值为 -100。
3. **reduction:** 指定损失的计算方式。可选值为：
   - `'none'`: 不进行任何降维，保留每个样本的损失（默认）。
   - `'mean'`: 将每个样本的损失取平均值。
   - `'sum'`: 将每个样本的损失相加。

## 2.3 MSELoss

**MSELoss**，**均方差损失函数**是用于回归问题的损失函数，它计算预测值与目标值之间的平均平方差。
$$
\text{MSELoss}(y_{true}, y_{pred}) = \frac{1}{N}\sum_{i=1}^N(y_{true} - y_{pred})^2
$$
在 PyTorch 中，可以使用 `nn.MSELoss` 类来定义均方误差损失。

```python
class torch.nn.MSELoss(reduction='mean')
```

- `reduction`：指定损失的计算方式，可以是以下值之一：
  - `'none'`：不进行任何缩减，返回每个样本的损失。
  - `'mean'`：返回所有样本损失的均值。
  - `'sum'`：返回所有样本损失的总和。

## 2.4 L2Loss

**L2Loss**，`L2`损失又被称为欧氏距离，是一种常用的距离度量方法，通常用于度量数据点之间的相似度。
$$
\text{L2}(y_{true},y_{pred})=\sqrt{\frac{1}{N}\sum_{i=1}^N(y_{true} - y_{pred})^2}
$$
代码实现：

```python
sqrt(nn.MSELoss(y1, y2))
```

## 2.5 L1Loss

**L1Loss**，**L1损失**又称为曼哈顿距离，表示残差的绝对值之和。L1损失函数对离群点有很好的鲁棒性，但它在残差为零处却不可导。
$$
\text{L1}(y_{true},y_{pred}) = \sum_{i=1}^N |y_{true}-y_{pred}|	\\
$$
在 PyTorch 中，可以使用 `nn.L1Loss` 类来**L1损失**。

```python
class torch.nn.L1Loss(reduction='mean')
```

`reduction`：指定损失的计算方式，可以是以下值之一：

- `'none'`：不进行任何缩减，返回每个样本的损失。
- `'mean'`：返回所有样本损失的均值（默认）。
- `'sum'`：返回所有样本损失的总和。

## 2.6 KL散度

**KL散度（ Kullback-Leibler divergence）**也被称为**相对熵**，是一种非对称度量方法，常用于度量两个概率分布之间的距离。`KL`散度也可以衡量两个随机分布之间的距离，两个随机分布的相似度越高的，它们的`KL`散度越小，当两个随机分布的差别增大时，它们的`KL`散度也会增大，因此KL散度可以用于比较文本标签或图像的相似性。
$$
KL(y_{true},y_{pred})
= \sum_{i=1}^N y_{true} \times log(\frac{y_{true}}{y_{pred}})
= \sum_{i=1}^N  y_{\text{true}} \cdot (\log y_{\text{true}} - \log y_{\text{pred}})
$$
在 PyTorch 中，可以使用 `nn.KLDivLoss` 类来**KL散度损失**。

```python
class torch.nn.KLDivLoss(reduction='mean')
```

`reduction`：指定损失的计算方式，可以是以下值之一：

- `'none'`：不进行任何缩减，返回每个样本的损失。
- `'mean'`：返回所有样本损失的均值（默认）。
- `'sum'`：返回所有样本损失的总和。
- `'batchmean'`：返回批量损失的均值。

