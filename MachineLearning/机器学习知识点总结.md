[toc]

#### 1、KMeans和KNN有什么区别？

**KMeans（K均值聚类）:**

1. **类型：** KMeans 是一种聚类算法，用于将数据集划分为 K 个不同的组（簇），使得每个数据点都属于离它最近的簇。
2. **目标：** 目标是找到 K 个簇的中心，以便最小化数据点到其所属簇中心的平方距离之和。
3. **工作原理：** 通过迭代优化，将数据点分配到最近的簇，并更新簇中心，直到满足停止准则（例如，簇中心不再变化或达到最大迭代次数）。
4. **应用：** 主要用于无监督学习中的聚类任务，将相似的数据点划分到同一簇。

**KNN（K最近邻）:**

1. **类型：** KNN 是一种分类和回归算法，可用于分类任务和回归任务。==在有监督学习中，KNN 主要用于分类任务。在无监督学习中，KNN 也可以用于聚类任务==。
2. **目标：** 对于分类任务，目标是根据最近邻的标签将新数据点分到不同的类别。对于回归任务，目标是根据最近邻的值进行预测。
3. **工作原理：** 对于给定的新数据点，KNN 查找与其最近的 K 个训练数据点，然后通过多数投票（对于分类）或平均值（对于回归）确定新数据点的标签或值。
4. **应用：** 可用于分类和回归任务，属于懒惰学习算法，不对训练数据进行显式的学习，而是在预测时直接使用训练数据。

**区别总结：**

- **任务类型：** KMeans 用于聚类，KNN 用于分类和回归。
- **目标：** KMeans 目标是找到数据集中的簇中心，使得数据点到簇中心的距离最小；KNN 目标是基于最近邻进行分类或回归。
- **学习方式：** KMeans 是无监督学习算法，KNN 可以是有监督或无监督，但常用于监督学习。
- **工作原理：** KMeans 通过迭代优化找到簇中心；KNN 在预测时直接使用最近邻信息。

#### 2、常见的监督和无监督算法有哪些？

监督学习算法：监督学习算法需要训练数据集中包含输入和对应的输出（或标签）信息。常用的监督学习算法包括：**线性回归、逻辑回归、决策树、支持向量机、朴素贝叶斯、人工神经网络**等。

无监督学习算法：无监督学习算法不需要训练数据集中的输出信息，主要用于数据的聚类和降维等问题。常用的无监督学习算法包括：**K均值聚类、层次聚类、主成分分析**等。

#### 3、偏差和方差

偏差（`Bias`）： 偏差是模型的预测值与实际值之间的差异，描述了模型的拟合能力。高偏差表示模型可能过于简单，无法很好地拟合训练数据，导致在训练集和测试集上都表现不佳。高偏差的模型通常欠拟合。

方差（`Variance`）： 方差是模型对训练数据的敏感性，即模型在不同训练数据上的预测值的变化程度。高方差表示模型过于复杂，对训练数据的小变化非常敏感，导致在训练集上表现良好但在测试集上表现差。高方差的模型通常过拟合。

这两者通常被称为“偏差-方差权衡”：

低偏差和高方差： 模型非常灵活，但容易受到噪声的影响，可能过拟合。

高偏差和低方差： 模型较为简单，但可能无法捕捉数据的复杂结构，可能欠拟合。

适度偏差和适度方差： 找到合适的平衡，使模型既能很好地拟合数据，又能在新数据上进行泛化。

改善模型性能的方法包括：

增加模型复杂度： 通过使用更复杂的模型（如深度神经网络）减小偏差，但可能增加方差。

减小模型复杂度： 通过使用更简单的模型（如线性模型）减小方差，但可能增加偏差。

正则化： 通过添加正则化项来平衡模型的复杂度，避免过拟合。

#### 4、硬间隔和软间隔

**硬间隔：**

- 硬间隔`SVM`的目标是在训练数据上找到一个线性划分，使得所有的训练样本都位于超平面的正确一侧，并且距离超平面的距离足够大，即不允许有任何训练样本落在间隔（`margin`）内。
- 硬间隔`SVM`要求训练数据是线性可分的，即存在一个超平面可以将正例和负例完全分开。

**软间隔：**

- 软间隔`SVM`允许在寻找最佳分割超平面时，一些训练样本可以位于间隔内。它引入了一个松弛变量（`slack variable`），允许一些样本违反硬间隔的规则。
- 软间隔`SVM`的目标是找到一个线性划分，同时最小化间隔内的样本数目和违背硬间隔规则的样本的程度。这种权衡可以通过正则化参数进行调整。
- 软间隔`SVM`适用于训练数据中包含噪声或有一些异常点的情况，或者当数据不是完全线性可分时。

#### 5、介绍一下分布函数

分布函数通常是指累积分布函数（`Cumulative Distribution Function`，简称`CDF`）。累积分布函数描述了随机变量小于或等于特定值的概率。

以下是累积分布函数的一些特点：

1. **非减性：** `CDF`是非减函数，即在整个定义域上，随着自变量的增加，`CDF`的值不减少。
2. **有界性：** `CDF`的取值范围在` [0, 1] `之间，因为概率的取值范围是` [0, 1]`。
3. **右连续性：** `CDF`是右连续的，即在定义域内的任何点，右侧极限等于该点的函数值。
4. **概率性质：** `CDF`在某个特定点的函数值表示了随机变量小于或等于该点的累积概率。即，$F(x)=P(X <= x)$，其中 $F(x)$ 是在点 $x$ 处的累积分布函数值，$P(X<=x)$是随机变量$X$小于或等于 $x$的概率。
5. **单调性：** 如果随机变量$X$的取值 $x_1<x_2$，则 $F(x_1)<=F(x_2)$，即`CDF`是单调不减的。

#### 6、常见的分布函数

1. **正态分布（Normal Distribution）：** 

   概率密度函数（PDF）：
   $$
   f(x|\mu,\sigma)=\frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
   $$
   其中$μ $是均值，$σ $是标准差。

2. **均匀分布（Uniform Distribution）：**

    概率密度函数（PDF）：
   $$
   f(x|a,b)=\frac{1}{b-a}, for \ \ \  a <= x <= b
   $$

3. **指数分布（Exponential Distribution）：**

    概率密度函数（PDF）：
   $$
   f(x|\lambda)=\lambda e^{-\lambda x}, for \ \ \ x >= 0
   $$
   $\lambda$ 是速率参数。

4. **伯努利分布（Bernoulli Distribution）：** 

   概率质量函数（PMF）： 
   $$
   f(x|p)=p^x(1-p)^{1-x}, for \ \ \ x \in \{0, 1\}
   $$
   其中，$p$是成功的概率。

5. **二项分布（Binomial Distribution）：**

    概率质量函数（PMF）： 
   $$
   f(x|n,p) = \begin{pmatrix}
      n \\
      x
   \end{pmatrix}p^x(1-p)^{n-x}, for \ \ \ x = 0,1,2,...,n
   $$
   其中，$n$是试验次数，$p$ 是成功的概率。

6. **泊松分布（Poisson Distribution）：**

    概率质量函数（PMF）： 
   $$
   f(x|\lambda) = \frac{e^{-\lambda}\lambda^x}{x!}, for \ \ \ x = 0,1,2,...
   $$
   其中，$\lambda$是事件发生的平均次数。

7. **伽玛分布（Gamma Distribution）：** 

   概率密度函数（PDF）：
   $$
   f(x|k,\theta) = \frac{1}{\Gamma(k)\theta^k}x^{k-1}e^{-\frac{x}{\theta}}, for \ \ \ x >= 0
   $$
   

   其中，$k$ 是形状参数，$\theta$ 是尺度参数。

8. **贝塔分布（Beta Distribution）：**

    概率密度函数（PDF）：
   $$
   f(x|a,b) = \frac{x^{a-1}(1-x)^{b-1}}{B(a,b)}, for \ \ \ 0 <=x<=1
   $$
   其中，$B(a,b) $是贝塔函数：
   $$
   B(a,b) = \int_0^1 t^{a-1}(1-t)^{b-1}dt
   $$
   贝塔函数也与伽玛函数有一定的联系，可用伽玛函数来表示：
   $$
   B(a,b)=\frac{\Gamma(a)\Gamma(b)}{\Gamma{(a+b)}}
   $$
   计算伽玛函数：
   $$
   \begin{aligned}
   \Gamma(n) &= (n-1)!	\\
   \Gamma(n) &= (n-1)\Gamma(n-1)	\\
   \Gamma(2) &= 1
   \end{aligned}
   $$
   









