[toc]



# Code-to-Text评估

[数据地址](https://huggingface.co/datasets/google-research-datasets/mbpp/tree/main/sanitized)。

```python
import os
import json
import requests

import pandas as pd

import anthropic
from zhipuai import ZhipuAI
from openai import OpenAI
# from volcenginesdkarkruntime import Ark


'''
    随机抽取数据集
'''
def random_sample_data(file_path, sample_num=20):
    save_file_path = "D:\\workspace\\VSCode\\test\\model_evaluate\\dataset\\dataset.csv"
    if os.path.exists(save_file_path):
        return pd.read_csv(save_file_path)
    
    df = pd.read_parquet(file_path)
    sample_data = df.sample(n=sample_num)
    sample_data.to_csv(save_file_path, index=False)
    return sample_data


'''
    chatglm4-air 模型 推理 评估
'''
def glm4_inference(api_key, prompt):
    client = ZhipuAI(api_key=api_key) 
    response = client.chat.completions.create(
        model="glm-4-plus",  
        messages=[
            {'role': 'system', 'content': prompt['system']},
            {"role": "user", "content": prompt['user']},
        ],
        stream=False,
        temperature=0.1
    )
    return response.choices[0].message.content


'''
    deepseek 模型评估
'''
def deepseek_inference(api_key, prompt):
    client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")
    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": prompt['system']},
            {"role": "user", "content": prompt['user']},
        ],
        stream=False,
        temperature=1.0
    )
    # print(response.choices[0].message.content)
    return response.choices[0].message.content


'''
    豆包 模型推理和评估
'''
def doubao_inference(api_key, prompt):
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    data = {
        "model": "ep-20250108150016-rhlt2",
        "messages": [
            {"role": "system", "content": prompt['system']},
            {"role": "user", "content": prompt['user']},
        ]
    }
    response = requests.post(
        "https://ark.cn-beijing.volces.com/api/v3/chat/completions", 
        headers=headers, 
        data=json.dumps(data)
    )
    return response.json()['choices'][0]['message']['content']

'''
    claude 模型推理和评估
'''
def claude_inference(api_key, prompt):
    client = anthropic.Anthropic(
        api_key=api_key,
    )
    
    message = client.messages.create(
        model="claude-3-5-sonnet-20241022",
        max_tokens=1024,
        messages=[
            {"role": "system", "content": prompt['system']},
            {"role": "user", "content": prompt['user']}
        ]
    )
    print(message.content)

def evaluate(
        inference_model, 
        inference_api_key,
        inference_prompt,
        evaluate_model,
        evaluate_api_key,
        evaluate_prompt
):
    if inference_model == 'glm4':
        inference_result = glm4_inference(inference_api_key, inference_prompt)
    elif inference_model == 'deepseek':
        inference_result = deepseek_inference(inference_api_key, inference_prompt)
    elif inference_model == 'doubao':
        inference_result = doubao_inference(inference_api_key, inference_prompt)

    code = inference_prompt['user']
    evaluate_prompt['user'] = f'代码：{code}\n\n代码解释：{inference_result}'

    if evaluate_model == 'glm4':
        evaluate_result = glm4_inference(evaluate_api_key, evaluate_prompt)
    elif evaluate_model == 'deepseek':
        evaluate_result = deepseek_inference(evaluate_api_key, evaluate_prompt)
    elif evaluate_model == 'doubao':
        evaluate_result = doubao_inference(evaluate_api_key, evaluate_prompt)

    return inference_result, evaluate_result, evaluate_prompt


if __name__ == "__main__":
    dataset = random_sample_data("./model_evaluate/dataset/test-00000-of-00001.parquet")

    model = ['glm4', 'deepseek', 'doubao', 'claude']
    # api_key
    api_keys = {
        'glm4': 'your-api-key',
        'deepseek': 'your-api-key',
        'doubao': 'your-api-key',
        'claude': 'your-api-key'
    }

    inference_model = 'deepseek'
    evaluate_model = 'deepseek'
    
    save_json_path = 'D:\\workspace\\VSCode\\test\\model_evaluate\\evaluate_results\\prompt_improvement\\'
    records = []

    for i, item in enumerate(dataset.iterrows()):
        prompt = item[1]['prompt']
        code = item[1]['code']

        inference_prompt = {
            "system": f"你是一个强大的代码解释助手，你需要抽取并解释代码每一行的作用。如果代码存在异常和安全风险，请指出并给出建议。最后用一句话解释该函数的功能。",
            "user": code
        }
        evaluate_prompt = {
            "system": f"你是一个代码解释评估专家，你需要评估给出的 '代码' 和 '代码解释' 是否相符，评估标准包括：\n 1、代码抽取能力，必须准确无误，否则0分。\n2、代码逐行解释能力，每解释错误一行扣1分。\n3、代码的异常和安全风险识别，如果代码有风险但并没有识别到，扣2分。\n\n最终给出范围在[0,10]的评分，并给出这样打分的理由。\n回答格式如下：\n\n评分：[分数]\n\n评估理由：[理由]",
        }
        inference_result, evaluate_result, ret_evaluate_prompt = evaluate(
            inference_model, 
            api_keys[inference_model],
            inference_prompt,
            evaluate_model,
            api_keys[evaluate_model],
            evaluate_prompt
        )

        record = {
            'id': i,
            'code': code,
            'prompt': prompt,
            'inference_prompt': inference_prompt,
            'inference_result': inference_result,
            'evaluate_prompt': ret_evaluate_prompt,
            'evaluate_result': evaluate_result
        }
        print(record)
        records.append(record)
    with open(
        f'{save_json_path}{inference_model}-{evaluate_model}.json', 
        'a+', 
        encoding='utf-8'
    ) as f:
        json.dump(records, f, indent=4, ensure_ascii=False)
```

# 评估结果

## 颜雪的数据

特别的数据：

```markdown
### 进程信息
11949 root      20   0 4600060   7888      4 S 807.5  0.0   7:20.83 dbused
12483 root      20   0 5915064   5076    988 S 807.2  0.0   3:44.83 kdevtmpfsi
17674 gdm       20   0 5689968 157692  54932 S   1.6  0.2   3:08.55 gnome-shell
4934 polkitd   20   0  546244  14692   5340 S   0.0  0.2   7:04.41 polkitd
```

### 评估结果：

`DeepSeek`和`Doubao`两个模型大部分给出的都是10分，但`Doubao`存在计算总分错误的情况。

在对特殊数据进行评估时：

`DeepSeek-DeepSeek`：推理错误，但评估时发现了错误，并纠正了过来，纠正的结果与`ChatGPT`给出的大部分结果一致。

`DeepSeek-Doubao`：推理错误，`Doubao`评估时给了满分。

`Doubao-Doubao`：由于不是代码所以没有详细推理，评估给了`0`分。`Doubao`出现计算总分错误的情况。

`Doubao-DeepSeek`：推理错误，但是却给了`10`分。

## metasploit的代码数据

### 评估结果

几乎全都给了满分，但`Doubao`依旧存在计算总分错误的情况。

## JS混淆代码数据

`DeepSeek-DeepSeek`：

- 出现了推理时没有发现风险，但评估时给了满分的情况。
- 推理时发现了风险，但评估给了0分。
